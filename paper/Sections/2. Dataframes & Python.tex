\subsection{Dataframes History}

Dataframes were initially introduced in the S programming language at Bell Laboratories in 1990 to facilitate statistical computations. The concept of dataframes was presented by Chambers, Hastie, and Pregibon at the Computational Statistics conference. They described dataframes as a class of objects in S that can conveniently organize variables relevant to specific analyses \cite{chambers1990statistical}. Subsequently, Chambers and Hastie expanded on this idea in a book published in 1992 \cite{chambers1992statistical}. In the book, they emphasized that data frames are more flexible than matrices because matrices in S assume all elements to be of the same type, whereas data frames can handle variables of different types. Additionally, data frames enable matrix-like computation with columns as variables and rows as observations and allow computations in which variables are treated separately and accessed by name. The R programming language, which is an open-source implementation of S, was released in 1995 and quickly gained popularity among statisticians. In 2008, Wes McKinney developed pandas, a Python library, aiming to bring dataframe capabilities with R-like semantics to Python, leading to its widespread adoption.\cite{chambers1992statistical}. In 2011 Wes McKinney introduced Pandas, a Python library designed to work with structured data sets, providing rich data structures and tools for data manipulation and analysis. It aims to be a foundational layer for statistical computing in Python and complements the existing scientific Python stack. Pandas address the need for rich data structures and metadata handling through its dataframe object, which allows for flexible and intuitive manipulation of labeled data sets, ensures automatic data alignment, and supports hierarchical indexing for advanced representation of higher-dimensional data within a 2D dataframe \cite{McKinney2011}.


\subsection{Dataframe Data Model}

Chambers and Hastie acknowledge that dataframes are distinct from familiar mathematical objects. Dataframes do not neatly fit into the categories of relations, matrices, or tensors. Instead, they adopt relational terminology from Abiteboul et al. and modify it to suit their purposes \cite{chambers1990statistical}. Dataframes consist of elements from a known set of domains, denoted as \( \text{Dom} = \{\text{dom1, dom2, ...}\} \). For simplicity, the discussion assumes domains are taken from \( \text{Dom} = \{\Sigma^*, \text{int, float, bool, category}\} \), although other domains like date-times are also common in practice. The domain \( \Sigma^* \) represents finite strings over an alphabet \( \Sigma \) and serves as a default, uninterpreted domain (sometimes called Object in certain dataframe libraries). Each domain contains a special null value (NA), and each domain \( \text{domi} \) is associated with a parsing function \( p_i: \Sigma^* \rightarrow \text{domi} \), allowing the interpretation of values in dataframe cells as domain values \cite{abiteboul1995foundations}\cite{petersohn2020scalable}.

A significant feature of dataframes is that the domains of their columns can be induced from data after data acquisition, rather than being declared in advance as in the relational model. They introduce a schema induction function \( S : (\Sigma^*)^m \rightarrow \text{Dom} \) that assigns an array of \( m \) strings to a domain in \( \text{Dom} \). This function is applied to a given column and returns a domain that describes the array of strings in that column. If the domain for any column is not specified, it can be inferred by applying \( S(\cdot) \) to the corresponding column in the data array \( A_{mn} \) \cite{abiteboul1995foundations}\cite{petersohn2020scalable}.

In summary, a dataframe is defined as a tuple \((A_{mn}, R_m, C_n, D_n)\), where \(A_{mn}\) is an array of entries from the domain \(\Sigma^*\), \(R_m\) is a vector of row labels from \(\Sigma^*\), \(C_n\) is a vector of column labels from \(\Sigma^*\), and \(D_n\) is a vector of \(n\) domains from \(\text{Dom}\), one for each column, with the possibility of being left unspecified. The schema of the dataframe is represented by \(D_n\), and if any domain in \(D_n\) is unspecified, it can be induced using the schema induction function \(S(\cdot)\) on the corresponding column of \(A_{mn}\) \cite{abiteboul1995foundations}\cite{petersohn2020scalable}.

Dataframes differ from traditional relational models in the way row and column labels are handled, and they also allow the possibility of defining schemas on rows as well as columns, providing unique cell interpretations and flexibility of interpretation in the algebra. When the schema \(D_n\) has the same domain \(dom\) for all \(n\) columns, they call it a homogeneous dataframe. A special case is the matrix dataframe, which adheres to the properties of a matrix, including support for linear algebra operations when it contains numeric values and operators such as \(+\) and \(\times\) forming a field. Overall, dataframes have elements from both relational and matrix viewpoints, but they are not equivalent to tables or matrices, showcasing unique characteristics and capabilities. The authors intend to use these perspectives to define both relational and linear algebra operations in their dataframe algebra for modern data science work \cite{abiteboul1995foundations}\cite{petersohn2020scalable}.

\subsection{Dataframe Limitations}

Dataframes can be mostly found in the popular Python library named Pandas. Although pandas is praised for its extensive API, it also contains significant redundancies among its operators, leading to varied performance implications. This complexity places a burden on users who need to manually plan queries by selecting the appropriate pandas API calls. For instance, a single task can be achieved using multiple different methods, with performance ranging from very fast to considerably slow. The panda's documentation itself provides several recommendations to improve performance. Consequently, many users opt to use only a small subset of operators, avoiding the bulk of the API. The intricacies of the API and its evaluation semantics make it challenging to apply traditional query optimization techniques. Each operator within a pandas "query plan" is executed entirely before subsequent operators, lacking extensive optimization, reordering, or pipelining, unless explicitly instructed by the user using .pipe. Furthermore, when dealing with even moderately large datasets that exceed memory capacity, the performance of the pandas.DataFrame API declines significantly. This is due to pandas' eager evaluation approach, where intermediate data sizes often surpass memory limits and require paging \cite{petersohn2020scalable}. Regarding memory usage and handling large datasets, pandas is currently limited to in-memory data sets. However, there is a desire to enhance its capabilities to accommodate data sets that surpass memory limits. One approach under consideration involves enabling pandas to seamlessly utilize the multiprocessing module or a parallel computing backend to facilitate large-scale computations without burdening the user with the complexities of managing memory constraints \cite{McKinney2011}.

Moreover, Pandas has several limitations that can impact its effectiveness in handling large datasets. One major concern is its memory usage, as it stores the entire dataset in memory, causing issues with datasets that exceed available memory and leading to performance bottlenecks. Pandas' memory usage is a widely recognized drawback attributed to its internal memory requirements. As suggested by McKinney, the creator of Pandas, it is advisable to have 5 to 10 times more RAM than the dataset's size to mitigate this issue \cite{sinthong2019aframe}. Additionally, complex operations like group by and join can be slow on large DataFrames, although some performance improvements can be achieved with vectorized operations, Cython, or Numba. Nonetheless, other libraries like Dask or Vaex are better suited for efficiently handling larger datasets. Another drawback is Pandas' lack of native support for parallel or distributed computing, which makes it less suitable for large-scale data processing tasks. Although libraries like Dask and Modin extend Pandas' functionality for parallel and distributed computing, they may have their own limitations. As a result, users dealing with larger datasets or working in distributed environments may find alternative libraries like Dask, Vaex, or Apache Spark more appropriate, offering improved performance and scalability for their data processing needs \cite{nelluri2021limitations}. The mentioned alternative libraries, such as Dask, Vaex, and Apache Spark, may present increased complexity and demand advanced programming skills and hardware setup, including cloud infrastructure. Unlike the relatively straightforward usage of Pandas, these libraries have steeper learning curves due to their distributed computing capabilities and specialized features. Working with Dask, Vaex, or Apache Spark may require a deeper understanding of distributed computing concepts, parallel processing, and data shuffling, making them more suitable for experienced data engineers and advanced programmers. Additionally, users may need to set up and configure cloud-based infrastructure to distribute workloads and process data in parallel across multiple nodes or virtual machines.

Some additional general drawbacks of the programming language approach like Python include the following. One of the drawbacks is that end-users who are not proficient in programming may find it challenging to explore data directly, as they would with graphical data exploration tools. Moreover, the lack of reusability in the approach can lead to duplication of effort and maintenance challenges when creating new datasets or performing different analyses. When data sources undergo changes or new data sources are introduced, it becomes challenging to update the code efficiently to accommodate these modifications. This difficulty arises due to the need for manual intervention and adjustments in the existing codebase, which can be time-consuming and error-prone. As data structures and formats evolve, the code must be adapted to handle these alterations, and when new data sources are integrated, the codebase may require significant reworking or rewriting. Additionally, maintaining and updating multiple instances of non-standardized code can become complex and error-prone, making it harder to ensure consistent data processing across the organization. This lack of reusability can hinder collaboration, scalability, and overall efficiency in data-driven projects. 

