Modern analytics environments grapple with the complexity of managing diverse datasets originating from various applications or collected by autonomous agents. Over the last decade, Business Analytics (BA) has evolved significantly through four eras: Analytics 1.0 to Analytics 4.0. In Analytics 1.0, traditional analytical tools were used for decision-making until 2005. The emergence of Big Data marked Analytics 2.0, led by companies like Google and Facebook. Analytics 3.0 combined Big Data and small data analytics for decision support and product creation, while Analytics 4.0, the current era, emphasizes autonomy and democratizing analytics tasks with new job roles like citizen data scientists and business translators alongside quantitative analysts and data scientists \cite{Aleksic2019}. The definition of big data has evolved rapidly, causing confusion among executives due to varying perspectives on its characteristics. The Three V's - Volume, Variety, and Velocity - serve as a common framework to describe big data, representing its high-volume, high-velocity, and high-variety nature. Other dimensions like Veracity, Variability, and Value have also been considered in defining big data. Universal benchmarks do not exist, and the definition depends on factors such as the firm's size, sector, and location. The Three-V tipping point signifies when traditional data management and analysis technologies become inadequate for handling big data efficiently \cite{GANDOMI2015137}. The high-variety datasets exhibit heterogeneity not only in their semantics but also in the data structures that store them and the systems that manage and process them. Utilizing Big Data poses significant challenges for managers across different business functions. To leverage its potential, a new profession, data scientist, with specific skill sets is necessary. This heterogeneity has created the need for different roles in the analytics environment including developers, data engineers, data scientists, data analysts, business users, and data regulation officers, that undertake different analysis projects, ranging from traditional business intelligence to data exploration, data mining, and prediction \cite{Mauro2016}. As of 2016, there was a shortage of approximately 190,000 data scientists in the United States alone \cite{Aleksic2019}.

To effectively address the challenges posed by this diverse landscape, multiple data systems and analysis platforms must coexist, integrated, and federated. While data warehousing has been a common approach, it becomes rigid in rapidly changing data environments. Furthermore, there is a need for techniques that allow the creation of late-bound schemas on data that may be persisted but rarely processed or even never processed. This requirement becomes particularly prevalent in dynamic data infrastructures where analysis objectives change with agility. In such environments, creating comprehensive classical integrated schemas encompassing structured, semi-structured, and unstructured data proves to be not only highly time and resource-intensive but often unattainable within the time constraints of analysis \cite{abadi2016beckman}. As an alternative, many production environments adopt a programming language (e.g., Python, R, Scala) to ad-hoc extract, transform, and assemble data, such as in dataframes, to swiftly build models or reports. Python is a popular open-source programming language supported on various platforms. It offers thousands of third-party packages, including NumPy, Scikit, and Pandas, which are essential for machine learning and data mining tasks. These packages provide support for scientific computing, data preprocessing, modeling, and analysis, making Python user-friendly and suitable for quick problem analysis \cite{Bhadani2016}. Although dataframe libraries in R and Python have achieved considerable success, they encounter performance problems when dealing with moderately large datasets. Additionally, there is considerable uncertainty about the precise meaning or interpretation of dataframe semantics \cite{petersohn2020scalable}. Moreover, this approach lacks data exploration capabilities for end-users, as generating new datasets necessitates creating entirely new programs. 

Python and R offer support for dataframe abstraction, which is a functional interface better suited for developers and data scientists implemented through data science notebooks. Dataframes are more tolerant of unknown data structures and are widely used in data exploration. They possess several characteristics that make them attractive for this purpose: intuitive data model, query language versatility, dataframes offer a query language that bridges various data analysis modes, including relational operations (e.g., filter, join), linear algebra (e.g., transpose), and spreadsheet-like functions (e.g., pivot), incrementally composable query syntax and native embedding in host languages \cite{perez2015project}. Pandas, the library within Python, has been a popular choice for data exploration. However, the rich API of pandas has led to redundancies, making it challenging for users to manually plan queries and optimize performance. The complexity of the API also hinders traditional query optimization techniques. Additionally, pandas' performance breaks down when processing moderate volumes of data that exceed memory limits \cite{petersohn2020scalable}.

As a response to that issue, we propose the following research question.
\textbf{Research question:} \textit{Can a virtual data machine that works on top of the organization data infrastructure as a layer that establishes on-demand connections with the required data sources offer easy dataframe queries formulation by no programming expert users and competitive performance to other traditional python-notebook-based solutions?}

To tackle this challenge, we propose the Data Virtual Machine (DVM), a novel graph-based conceptual model based on entities and attributes, concepts that users intuitively understand. The fundamental idea behind the DVM is both simple and powerful: given a computation C with an output (o1, o2), where o1 belongs to attribute domain A and o2 belongs to attribute domain B, the output of C can be represented as a mapping between A and B. This mapping can be depicted in a graph with nodes A and B and edges representing the mappings as manifested by C's output, which can encompass queries, scripts, programs, and more. The DVM introduces agile and on-demand modeling capabilities, allowing end-users to easily define computations over the data sources, covering relational and non-relational data, streams, and stand-alone programs, visually. As a result, the DVM is automatically generated, reflecting a collection of computations based on their output. This approach diverges from traditional data integration techniques, where the focus is on settling on a fixed schema and then defining data processing tasks to populate or refresh it in data warehousing, or utilizing wrappers to bind data with a virtual schema in case of mediators/virtual databases. Instead, the DVM fits existing data to dynamically constructed schemas, providing greater flexibility and adaptability to evolving analysis requirements. Additionally, it facilitates easy and intuitive querying, enabling non-database experts, such as data scientists and statisticians, to express queries using a high-level query definition language similar to SQL. The DVM's visualization-driven approach effectively hides query optimization and structural details, enhancing usability.

This paper's contributions encompass the introduction of the DVM as a novel conceptual model, a declarative approach to dataframing in analytics environments, and the proposal of an algebraic framework for efficient query evaluation along with concurrency and parallelization optimizations in the calculations. A case study validates the effectiveness of DVMs and the visual DVM query language in effortlessly creating advanced dataframes analysis that would otherwise require programming skills. Moreover, a benchmarking of the DataMigler tool is performed against the traditional Python-notebook-based implementations that demonstrate the superior performance of DataMigler in the query evaluation.

The subsequent sections elaborate on key analytics environment concepts, present the DVM's formal definition and intuitive explanation, introduce DVM dataframe queries and the algebraic framework for their evaluation, discuss query evaluation and optimization, showcase a case study as a proof-of-concept, benchmarking of the tool, and conclude with limitations and future directions for this innovative approach.